# -*- coding: utf-8 -*-
"""computational statistics assignment 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UFoLBBQAaC01NmoV6os48zoUORhPDcaq
"""

from google.colab import drive
drive.mount('/content/drive')

# imporing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error
from sklearn.tree import plot_tree

"""1) Create a Monte Carlo simulation to illustrate the problem."""

import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt


n_obs = 100  # Number of observations
n_pred_list = np.arange(1, 101, 10)  # Increasing number of predictors from 1 to 100 in steps of 10
n_simulations = 100  # Number of simulations to run for each number of predictors

# Function to run Monte Carlo simulation
def run_simulation(n_obs, n_pred_list, n_simulations):
    r_squared_means = []

    for n_pred in n_pred_list:
        r_squared_values = []
        for _ in range(n_simulations):

            # Generate random predictors X and a random response Y
            X = np.random.randn(n_obs, n_pred)
            X = sm.add_constant(X)  # Add intercept
            beta = np.random.randn(n_pred + 1, 1)  # Random coefficients
            Y = X @ beta + np.random.randn(n_obs, 1)  # Generate Y with some noise

            # Fit linear regression model and calculate R-squared
            model = sm.OLS(Y, X).fit()
            r_squared_values.append(model.rsquared)

        # Calculate mean R-squared for this number of predictors
        r_squared_means.append(np.mean(r_squared_values))

    return r_squared_means

r_squared_means = run_simulation(n_obs, n_pred_list, n_simulations)

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(n_pred_list, r_squared_means, marker='o')
plt.xlabel('Number of Predictors')
plt.ylabel('Average R-squared')
plt.title('Monte Carlo Simulation: Effect of Number of Predictors on R-squared')
plt.grid(True)
plt.show()

"""
4) Find a real dataset to illustrate the problem and your fix.
"""

# Load the uploaded dataset
file_path_real_data = '/content/drive/MyDrive/Untitled folder/diabetes.csv'
df = pd.read_csv(file_path_real_data)

# Inspect the first few rows of the dataset and its shape
df.head(), df.shape

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import r2_score

df = pd.read_csv('/content/drive/MyDrive/Untitled folder/diabetes.csv')

X = df.drop('Outcome', axis=1)
y = df['Outcome']

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Create and fit the linear regression model
model = LinearRegression().fit(X_train, Y_train)

# Predict and calculate R-squared
Y_pred = model.predict(X_test)
r2 = r2_score(Y_test, Y_pred)

print(f'R-squared: {r2:.4f}')

# Calculate Adjusted R-squared
n = X_test.shape[0]  # Number of observations in the test set
p = X_test.shape[1]  # Number of predictors
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)


print(f'Adjusted R-squared: {adjusted_r2:.4f}')